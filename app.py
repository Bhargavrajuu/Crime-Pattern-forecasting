import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import warnings
import json
from typing import Dict, Tuple

# AI/ML Imports
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.ensemble import IsolationForest
import shap
import lime
import lime.lime_tabular

# Suppress warnings
warnings.filterwarnings("ignore")

# ================ CONFIGURATION ================
st.set_page_config(
    page_title="Hybrid Predictive Crime Dashboard",
    layout="wide",
    page_icon="üõ°Ô∏è"
)

# Internationalization
LANGUAGES = {
    "English": {
        "title": "Hybrid Predictive Crime Dashboard",
        "description": "State-wise crime forecasting with explainable AI",
        # ... other translations
    },
    "Spanish": {
        "title": "Panel Predictivo de Criminalidad",
        "description": "Pron√≥stico del crimen con IA explicable",
        # ... other translations
    }
}

# ================ CACHED FUNCTIONS ================
@st.cache_data
def load_data(state: str = None) -> pd.DataFrame:
    """Load and preprocess crime data with privacy-preserving measures"""
    # In production, this would connect to federated data sources
    df = pd.read_excel("AI PROJECT(2).xlsx")
    
    # Anonymization - remove direct identifiers
    df = df.drop(columns=['id', 'registration_circles'], errors='ignore')
    
    if state:
        df = df[df['state_name'] == state]
    return df

@st.cache_data
def train_models(data: pd.DataFrame, crime_type: str) -> Dict:
    """Train multiple forecasting models and return results"""
    results = {}
    
    # Prepare time series data
    ts_data = data.groupby('year')[crime_type].sum().reset_index()
    ts_data['ds'] = pd.to_datetime(ts_data['year'], format='%Y')
    ts_data['y'] = ts_data[crime_type]
    
    # ARIMA
    try:
        arima = ARIMA(ts_data['y'], order=(1,1,1)).fit()
        results['ARIMA'] = {
            'model': arima,
            'metrics': {'aic': arima.aic}
        }
    except Exception as e:
        st.error(f"ARIMA failed: {str(e)}")
    
    # Prophet
    try:
        prophet = Prophet()
        prophet.fit(ts_data[['ds', 'y']])
        results['Prophet'] = {
            'model': prophet,
            'metrics': {}
        }
    except Exception as e:
        st.error(f"Prophet failed: {str(e)}")
    
    # LSTM
    try:
        # Create sequences for LSTM
        values = ts_data['y'].values
        n_steps = 3
        X, y = [], []
        for i in range(len(values) - n_steps):
            X.append(values[i:i+n_steps])
            y.append(values[i+n_steps])
        X, y = np.array(X), np.array(y)
        
        # Build model
        model = Sequential([
            LSTM(50, activation='relu', input_shape=(n_steps, 1)),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        model.fit(X, y, epochs=200, verbose=0)
        
        results['LSTM'] = {
            'model': model,
            'metrics': {},
            'preprocessor': (X, y, n_steps)
        }
    except Exception as e:
        st.error(f"LSTM failed: {str(e)}")
    
    return results

# ================ COMPONENTS ================
def model_selection_panel(models: Dict) -> str:
    """Auto-select best model based on metrics"""
    # Simple selection logic - in production would use more sophisticated criteria
    if 'ARIMA' in models and 'Prophet' in models:
        if models['ARIMA']['metrics']['aic'] < 200:  # Example threshold
            return 'ARIMA'
        return 'Prophet'
    elif 'LSTM' in models and len(models) == 1:
        return 'LSTM'
    return list(models.keys())[0]

def explainability_panel(model, model_type: str, data: pd.DataFrame) -> None:
    """Generate explainability visualizations"""
    st.subheader("Model Explanation")
    
    if model_type == 'ARIMA':
        # SHAP explanation
        explainer = shap.Explainer(model.predict, data['y'])
        shap_values = explainer(data['y'])
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=shap_values.values,
            y=data['year'].astype(str),
            orientation='h'
        ))
        fig.update_layout(title="SHAP Values - Feature Importance")
        st.plotly_chart(fig)
        
    elif model_type == 'Prophet':
        # Prophet components
        fig = model.plot_components(model.predict(data[['ds', 'y']]))
        st.pyplot(fig)
    
    # Natural language explanation
    st.markdown(f"""
    **Model Rationale**:  
    The {model_type} model suggests this trend based on:
    - Historical patterns from {data['year'].min()} to {data['year'].max()}
    - Seasonal variations in crime rates
    - Confidence intervals calculated from past performance
    """)

def anomaly_detection(data: pd.DataFrame, forecast: pd.Series) -> None:
    """Detect and visualize anomalies"""
    clf = IsolationForest(contamination=0.1)
    values = data['y'].values.reshape(-1, 1)
    clf.fit(values)
    preds = clf.predict(values)
    
    anomalies = data[preds == -1]
    if not anomalies.empty:
        st.warning(f"‚ö†Ô∏è Detected {len(anomalies)} historical anomalies")
        fig = px.scatter(
            data, x='year', y='y',
            title="Anomaly Detection",
            color=preds,
            color_discrete_map={1: 'blue', -1: 'red'}
        )
        st.plotly_chart(fig)

# ================ MAIN APP ================
def main():
    # Role-based access control
    roles = ["Public", "Law Enforcement", "Policy Maker", "Researcher"]
    role = st.sidebar.selectbox("Select Your Role", roles)
    
    # Multi-language support
    lang = st.sidebar.selectbox("Language", list(LANGUAGES.keys()))
    strings = LANGUAGES[lang]
    
    st.title(strings["title"])
    st.markdown(strings["description"])
    
    # Data selection
    states = load_data()['state_name'].unique()
    selected_state = st.sidebar.selectbox("State", sorted(states))
    
    # Load state data with privacy controls
    state_data = load_data(selected_state)
    crime_types = [c for c in state_data.columns 
                  if c not in ['year', 'state_name', 'state_code', 'district_name']]
    selected_crime = st.sidebar.selectbox("Crime Type", crime_types)
    
    # Model training
    if st.sidebar.button("Run Analysis"):
        with st.spinner("Training models..."):
            models = train_models(state_data, selected_crime)
            best_model = model_selection_panel(models)
            
            # Display results
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Selected Model", best_model)
                st.metric("Data Points", len(state_data))
            with col2:
                st.metric("Time Period", 
                         f"{state_data['year'].min()} - {state_data['year'].max()}")
            
            # Generate forecast
            forecast_years = st.sidebar.slider("Forecast Years", 1, 10, 5)
            forecast_results = generate_forecast(models[best_model], best_model, forecast_years)
            
            # Visualization
            plot_forecast(state_data, forecast_results, selected_crime, selected_state)
            
            # Explainability
            explainability_panel(models[best_model]['model'], best_model, state_data)
            
            # Anomaly detection
            anomaly_detection(state_data, forecast_results['values'])
            
            # Policy recommendations (role-specific)
            if role != "Public":
                policy_recommendations(forecast_results, selected_state, role)

if __name__ == "__main__":
    main()
